<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <title>Long Chen's Homepage</title>

  <link rel="stylesheet" href="stylesheets/styles.css">
  <link rel="stylesheet" href="stylesheets/pygment_trac.css">
  <meta name="viewport" content="width=device-width">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
  <div class="wrapper">
    <header>
      <h1>Long Chen</h1>
      <p></p>
      <p>PhD Candidate @ Dept. of Computer Science & Technology, Tsinghua University</p>
      <img src="images/longchen.png" alt="Photo @ Tsinghua Sci. &amp; Tech. Park.">
      <!-- <ul>
          <li><a href="https://scholar.google.com/citations?user=FlIzsGQAAAAJ&hl=zh-CN"><strong>Google Scholar</strong></a></li>
          <li><a href="https://github.com/longcw">Download <strong>TAR Ball</strong></a></li>
          <li><a href="http://github.com/orderedlist/minimal">Fork On <strong>GitHub</strong></a></li>
        </ul> -->
      <p>
        <b>Email:</b> <a href="mailto:longch1024@gmail.com">longch1024@gmail.com</a><br>
        <b>Github:</b> <a href="https://github.com/longcw" , target="_blank">https://github.com/longcw</a><br>
        <!-- <b>Google Scholar:</b> <a href="http://t.cn/A67cLd1u" ,
          target="_blank">http://t.cn/A67cLd1u</a><br> -->
        <b>Google Scholar:</b> <a href="https://scholar.google.com/citations?user=FlIzsGQAAAAJ" ,
          target="_blank">https://scholar.google.com/citations?user=FlIzsGQAAAAJ</a><br>
        <b>LinkedIn:</b> <a href="https://www.linkedin.com/in/longch/" ,
          target="_blank">https://www.linkedin.com/in/longch</a><br>
      </p>


    </header>
    <section>
      <h1>About Me</h1>
      <p>I received my PhD in Computer Science from Tsinghua University in June 2021, 
        supervised by <a href="https://scholar.google.com/citations?user=hhV6J6UAAAAJ&amp;hl=en", target="_blank">Prof.
        Haizhou Ai</a>, 
        and my BE in Electronic Information Engineering from Huazhong University of Science and Technology in July 2016.
      </p>

      <h1>Education</h1>
      <h3>Tsinghua University, Beijing, China</h3>
      <ul>
        <li><b>PhD Candidate</b>, Dept. of Computer Science and Technology</li>
        <li><b>Research Interest:</b> Multi­Object Tracking, 3D Human Pose Estimation, Detection and Person
          Re-identification</li>
      </ul>

      <h3>Huazhong University of Science and Technology, Wuhan, China</h3>
      <ul>
        <li>Bachelor of Electronic and Information Engineering</li>
        <li>Rank: 1/175</li>
      </ul>

      <h1>Research Projects</h1>

      <h3>Multi-Person 3D Pose Estimation and Tracking from Multiple View</h3>
      <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/-4wTcGjHZq8" frameborder="0"
        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
      <ul>
        <li>Associating human keypoints across multiple cameras and frames to reconstruct and track
          people in 3D space. The method works in real­time and has been deployed to some realworld productions.</li>
        <li>Accepted by CVPR 2020 (<a href="https://arxiv.org/abs/2003.03972" , target="_blank">arxiv</a>)</li>
        <li>
          <a href="https://youtu.be/-4wTcGjHZq8" , target="_blank">Video Demo1</a>,
          <a href="https://youtu.be/Mw7OIHD4_B0" , target="_blank">Video Demo2</a>
        </li>
      </ul>

      <h3>Online / Near-Online Multi-Object Tracking</h3>
      <ul>
        <li>
          A series of tracking­by­detection approaches that designed for tracking multiple people in a
          monocular, unconstrained camera.
          The proposed methods achieved state­-of-­the­-art performance on <i>MOTChallenge</i>
          for both accuracy and speed at that time.
        </li>
        <li>
          The published papers have been cited by over 100 times.
        </li>
        <li>
          <a href="https://ieeexplore.ieee.org/document/8296360" , target="_blank">Paper1</a>,
          <a href="https://arxiv.org/abs/1809.04427" , target="_blank">Paper2</a>,
          <a href="https://github.com/longcw/MOTDT" , target="_blank">Code</a>,
          <a href="https://youtu.be/BzetI6wwO64" , target="_blank">Video Demo</a>
        </li>
      </ul>

      <h3>Single Camera Relative Depth Estimation with Synthetic Data</h3>
      <ul>
        <li>Obtianing relative depth from single image, trained using both real-world images from <i>NYU Depth</i> and
          synthetic
          images from the computer game <i>GTA-V</i>.</li>
        <li>
          <a href="https://youtu.be/Db_43BtWqNI" , target="_blank">Video1</a>,
          <a href="https://youtu.be/ES9rNkr40eQ" , target="_blank">Video2</a>,
          <a href="https://skfb.ly/6NwKO" , target="_blank">Online Demo</a>
        </li>
      </ul>

      <h3>Object Detection in Faster R-CNN and YOLO</h3>
      <ul>
        <li>Learning PyTorch along with Faster R­CNN and YOLOv2, implemented the first, or one of
          the firsts, open­source object detector that written in PyTorch.
          The most interesting part of this project is to read the
          source code of Torch & PyTorch and implement the RoIPooling using Cuda.</li>
        <li>The repositories on Github have recieved over 2000 stars in total. </li>


        <li>
          Codes of
          <a href="https://github.com/longcw/faster_rcnn_pytorch" , target="_blank">Faster R-CNN</a>,
          <a href="https://github.com/longcw/yolo2-pytorch" , target="_blank">YOLOv2</a>
        </li>
      </ul>

      <h1>Publications</h1>
      <p>
        <b>L Chen</b>, H Ai, R Chen, Z Zhuang, S Liu “Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100
        FPS”,
        CVPR , 2020.
        <a href="https://arxiv.org/abs/2003.03972" , target="_blank">[pdf]</a>
      </p>
      <p>
        <b>L Chen</b>, H Ai, R Chen, Z Zhuang, “Aggregate Tracklet Appearance Features for Multi-Object Tracking”,
        IEEE Signal Processing Letters, 2019.
        <a href="https://ieeexplore.ieee.org/document/8831410" , target="_blank">[pdf]</a>
      </p>
      <p>
        <b>L Chen</b>, H Ai, Z Zhuang, C Shang, “Real-time Multiple People Tracking with Deeply Learned
        Candidate Selection and Person Re-Identification”, IEEE International Conference on Multimedia
        and Expo, 2018.
        <a href="https://arxiv.org/abs/1809.04427" , target="_blank">[pdf]</a>
        <a href="https://github.com/longcw/MOTDT" , target="_blank">[code]</a>
      </p>
      <p>
        <b>L Chen</b>, H Ai, C Shang, Z Zhuang, B Bai, “Online Multi-Object Tracking with Convolutional
        Neural Networks”, IEEE International Conference on Image Processing, 2017.
        <a href="https://ieeexplore.ieee.org/document/8296360" , target="_blank">[pdf]</a>
      </p>
      <p>
        R Chen, H Ai, C Shang, <b>L Chen</b>, Z Zhuang, “Learning Lightweight Pedestrian Detector with
        Hierarchical Knowledge Distillation”, IEEE International Conference on Image Processing, 2019.
        <a href="https://ieeexplore.ieee.org/abstract/document/8803079" , target="_blank">[pdf]</a>
        <a href="https://github.com/RuiChen96/MaskRCNN-Knowledge-Distillation" , target="_blank">[code]</a>
      </p>
      <p>
        Z Zhuang, H Ai, <b>L Chen</b>, C Shang, “Cross-Resolution Person Re-identification with Deep Antithetical
        Learning”, Asian Conference on Computer Vision, 2018.
        <a href="https://arxiv.org/abs/1810.10221" , target="_blank">[pdf]</a>
      </p>



      <h1>Professional Skills</h1>
      <p><b>Reviewer</b> of <i>IEEE Transactions on Circuits and Systems for Video Technology</i></p>
      <p><b>Programming Language:</b> C/C++, Python</p>
      <p><b>Deep Learning Framework:</b> PyTorch, Tensorflow, Caffe</p>


    </section>
    <footer>
      <p>
        <small>
          Hosted on GitHub Pages &mdash;
          Theme by <a href="https://github.com/orderedlist" , target="_blank">orderedlist</a>.
          <!-- <span id="busuanzi_container_site_pv" style='display:none'>
            Viewed <span id="busuanzi_value_site_pv"></span> times.
          </span> -->
        </small>
      </p>
    </footer>
  </div>
  <script src="javascripts/scale.fix.js"></script>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>
</body>

</html>